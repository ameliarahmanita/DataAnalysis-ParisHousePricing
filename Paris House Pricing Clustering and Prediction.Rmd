---
title: "Study Case 3 - Clustering, Classification, and Prediction of House Pricing in Paris"
author: "IS388A - A2 Group"
date: "November 2021"
output: 
  html_document: 
    number_sections: yes
  pdf_document: 
    toc: yes
    toc_depth: 4
    number_sections: yes
    keep_tex: yes
  word_document: 
    toc: yes
    toc_depth: 4
    highlight: tango
    keep_md: yes
---

## Loading Library

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)

library(knitr) 
library(readxl) # Reading Excel
library(Amelia) # Missing Data : Missings Map
library(dplyr) # Data Manipulation
library(splitstackshape) # for stratified sampling
library(cluster) # avg silhouette
library(factoextra) # plotting
library(mclust) # Clustering : Model-based Clustering
library(ClusterR) # For external validation
library(klaR) # Classification : Naive Bayes
library(caret) # Making Confusion Matrix
library(randomForest) # Classification : Random Forest
library(stats) # Fitting linear models
library(car) # for multicollinearity checking
library(nortest) # for normality test
library(lmtest) # for homoscedasticity test
library(Metrics) # for calculating RMSE
```


## Preparing and Cleaning Data

"Paris Housing" data description.

The following is a data about the pricing of houses in Paris based on the house specification or conditions
We obtained this data from Kaggle (https://www.kaggle.com/aleshagavrilov/parishousing)

The dataset consists of 18 variables which are :

1.  Index : The index/numbering of the data (which we will remove later because we don't need it)
2.  Square Meters (squareMeters) : Area if the apartment in square meters
3.  Number of Rooms (numberOfRooms) : Number of rooms in the apartment
4.  Floors (floors) : The amount of floors in the house
5.  City Code (cityCode) : Code of the city, where the apartment situated
6.  City Part Range (cityPartRange) : Prestige of the area in range from 0 to 10
7.  Number of Previous Owners (numPrevOwners) : Number of previous owners
8.  Made (made) : The year when house was built
9.  New Built (isNewBuilt) : The aparment is new or renovated
10. Storm Protector (hasStormProtector) : Apartment has storm protector or not
11. Basement (basement) : The apartment has basement or not (in square meters)
12. Attic (attic) : The apartment has attic or not (in square meters)
13. Garage (garage) : The apartment has garage or not (in square meters)
14. Storage Room (hasStorageRoom) : The apartment has storage room or not
15. Guest Room (hasGuestRoom) : The number of guest room in the apartment
16. Price (price) : The price of the apartment
17. Category (category) : Category of the house
18. Pool and Yard (poolAndYard) : The apartment has pool and yard or not

The purpose of this project is to build clustering, classification, and prediction model of House Pricing in Paris using their respective algorithms.

```{r}
#--------------------------Data Introduction--------------------------
# Reading dataset and saving it into 'A2' variable
A2 <- read_excel("CS3_A2.xlsx", sheet = "CS3_A2")

# Checking the structure of the data
str(A2)

# Displaying the first few rows of the data
head(A2)

# Showing the summary of the data
summary(A2)

#------------------------Handling Missing Data------------------------
# Checking missing values (missing values or empty values) before Omitting NAs
colSums(is.na(A2) | A2 == '')

# Visualize the missing data before Omitting NAs
missmap(A2, legend = TRUE, main = "Visualize Missing Observation")

# In this data, there isn't any missing values.

#----------------------Eliminate Duplicate Data-----------------------
A2 <- distinct(A2)
str(A2)

#------------------------Manipulating Data Type-----------------------
# Discard the unused categorical variables.
A2_new <- A2 %>% dplyr :: select (-cityCode, -cityPartRange, -isNewBuilt, -hasStorageRoom, -hasStormProtector, -poolAndYard)
# Because, these are the categorical variables we don't need in clustering the data.

# Changing the char data type as factor
#----- category variable
A2_new$category <- as.factor(A2_new$category)
A2_new$category <- factor(A2_new$category, levels = c("Basic", "Luxury"))

# We need the category to validate the clustering model.

# Checking the class of each variable in the data (data type) after discarding some variables and changing char to factor
str(A2_new)
sapply(A2_new, class)

# the new data frame only consists of numerical variables and one categorical variables which is category to validate the clustering models.
```


## Data Visualization; Exploratory Data Analysis
```{r fig, fig.height = 5, fig.width = 10}
#----------------------------Visualization----------------------------
# Categorical type ---------------------------------------------------
tab1 <-table(A2_new$category)
barplot(tab1, main = "House Category Barplot", col = rainbow(2), ylim = c(0,10000))
# CONCLUSION : the frequency of basic is higher than the frequency of luxury house category.

# Categorical x Numerical type ---------------------------------------
par(mfrow = c(2,2))

bx1 <- boxplot(A2_new$squareMeters ~ A2_new$category, main = "House Category by Area Boxplot", xlab = "Category", ylab = "Area (in square meters)", col = rainbow(2))

bx2 <- boxplot(A2_new$numberOfRooms ~ A2_new$category, main = "House Category by Number of Rooms Boxplot", xlab = "Category", ylab = "Number of Rooms", col = rainbow(2))

bx3 <- boxplot(A2_new$floors ~ A2_new$category, main = "House Category by Floors Boxplot", xlab = "Category", ylab = "Floors", col = rainbow(2))

bx4 <- boxplot(A2_new$numPrevOwners ~ A2_new$category, main = "House Category by Number of Previous Owner Boxplot", xlab = "Category", ylab = "Number of Previous Owner", col = rainbow(2))

bx5 <- boxplot(A2_new$made ~ A2_new$category, main = "House Category by Year Boxplot", xlab = "Category", ylab = "Year Built", col = rainbow(2))

bx6 <- boxplot(A2_new$basement ~ A2_new$category, main = "House Category by Basement Boxplot", xlab = "Category", ylab = "Basement Area (in square meters)", col = rainbow(2))

bx7 <- boxplot(A2_new$attic ~ A2_new$category, main = "House Category by Attic Boxplot", xlab = "Category", ylab = "Attic Area (in square meters)", col = rainbow(2))

bx8 <- boxplot(A2_new$garage ~ A2_new$category, main = "House Category by Garage Boxplot", xlab = "Category", ylab = "Garage Area (in square meters)", col = rainbow(2))

bx9 <- boxplot(A2_new$hasGuestRoom ~ A2_new$category, main = "House Category by Guest Room Boxplot", xlab = "Category", ylab = "Guest Room", col = rainbow(2))

bx10 <- boxplot(A2_new$price ~ A2_new$category, main = "House Category by Price Boxplot", xlab = "Category", ylab = "Price", col = rainbow(2))

# CONCLUSION : All the boxplots for house category vs numerical variables are mostly similar between the basic and the luxury type.

# Checking the stats
# stats: each column represents the lower whisker, the first quartile, the median, the third quartile and the upper whisker of each group.
bx1$`stats` 
bx2$`stats`
bx3$`stats`
bx4$`stats`
bx5$`stats`
bx6$`stats`
bx7$`stats`
bx8$`stats`
bx9$`stats`
bx10$`stats`

# Checking the outlier
bx1$out
bx2$out
bx3$out
bx4$out
bx5$out
bx6$out
bx7$out
bx8$out
bx9$out
bx10$out

# Anderson-Darling test to check whether the distribution is Gaussian
nortest::ad.test(A2_new$squareMeters)
nortest::ad.test(A2_new$numberOfRooms)
nortest::ad.test(A2_new$floors)
nortest::ad.test(A2_new$numPrevOwners)
nortest::ad.test(A2_new$made)
nortest::ad.test(A2_new$basement)
nortest::ad.test(A2_new$attic)
nortest::ad.test(A2_new$garage)
nortest::ad.test(A2_new$hasGuestRoom)
nortest::ad.test(A2_new$price)
# Result : all the p-value < alpha (0.05), so we reject the h0 hypothesis
# Conclusion : The distribution is not Gaussian or normally distributed (Ha)
# So, we use a non-parametric alternative of t-test to compare paired data


# Test the means differences to check whether the differences are significant using Wilcoxon rank sum test
# Wilcoxon rank sum test is equivalent with Mann-Whitney U test
(wt1 <- wilcox.test(squareMeters ~ category, data = A2_new, paired = FALSE))
(wt2 <- wilcox.test(numberOfRooms ~ category, data = A2_new, paired = FALSE))
(wt3 <- wilcox.test(floors ~ category, data = A2_new, paired = FALSE))
(wt4 <- wilcox.test(numPrevOwners ~ category, data = A2_new, paired = FALSE))
(wt5 <- wilcox.test(made ~ category, data = A2_new, paired = FALSE))
(wt6 <- wilcox.test(basement ~ category, data = A2_new, paired = FALSE))
(wt7 <- wilcox.test(attic ~ category, data = A2_new, paired = FALSE))
(wt8 <- wilcox.test(garage ~ category, data = A2_new, paired = FALSE))
(wt9 <- wilcox.test(hasGuestRoom ~ category, data = A2_new, paired = FALSE))
(wt10 <- wilcox.test(price ~ category, data = A2_new, paired = FALSE))
# Result : all the p-value > alpha (0.05), so we cannot reject the h0 hypothesis
# Conclusion : The mean is equal or there is no significant difference between the means of variables (H0)
```

## Splitting the Data 
```{r}
#----------------------------Splitting Data---------------------------
# Split data Training and Testing 80 : 20
Rand <- 16098 # 5 digits
set.seed(Rand) # setting seed

# Using stratified sampling for splitting the data
trainData <- stratified(A2_new, c("category"), 0.8, replace = FALSE)
testData <- stratified(A2_new, c("category"), 0.2, replace = FALSE)

nrow(trainData)
table(trainData$category)

nrow(testData)
table(testData$category)

# #---------------------------------------------------------------------------------
# or we could use different method to do stratified sampling from caret package. it will give the same results
# train.index <- caret::createDataPartition(A2_new$category, p = .8, list = FALSE)
# train <- A2_new[ train.index,]
# test  <- A2_new[-train.index,]
# 
# table(train$category)
# table(test$category)
```

## Data Clustering : Using 2 Algorithms

### Clustering Algorithm 1 (k-Means)
```{r}
# no scaling ---------------------------------------------------------

#-----------DETERMINE THE OPTIMAL NUMBER OF CLUSTERS------------------
# Average Silhouette Method ------------------------------------------
# automatic plot with package factoextra
factoextra::fviz_nbclust(trainData[,1:10], FUNcluster = kmeans, method = "silhouette") +
  labs(subtitle = "Average Silhouette Method")

# k = 2 based on Avg Silhouette Method

#-----------------------CLUSTERING USING K-MEANS----------------------
km <- kmeans(trainData[,1:10], centers = 2, nstart = 25) 

#---------------------------PLOT THE CLUSTERS-------------------------
factoextra::fviz_cluster(km, data = trainData[,1:10],
             palette = c("#2E9FDF", "#00AFBB", "#E7B800"), 
             geom = "point",
             ellipse.type = "convex", 
             ggtheme = theme_bw()
)

#---------------------------CLUSTER VALIDATION------------------------
# External validation using Rand Index
# Compute the Corrected Rand Index
(cri_km <- external_validation(as.numeric(trainData$category), km$cluster, 
                          method = "adjusted_rand_index", summary_stats = T))

# Compute the Variation of Information
(vi_km <- external_validation(as.numeric(trainData$category), km$cluster, 
                          method = "var_info", summary_stats = F))


# scaling ------------------------------------------------------------
A2scaled <- scale(trainData[,1:10])

#-----------DETERMINE THE OPTIMAL NUMBER OF CLUSTERS------------------
# Average Silhouette Method ------------------------------------------
# automatic plot  with package factoextra
factoextra::fviz_nbclust(A2scaled, kmeans, method = "silhouette") +
  labs(subtitle = "Average Silhouette Method")

# k = 2 based on Avg Silhouette Method 

#-----------------------CLUSTERING USING K-MEANS----------------------
km1 <- kmeans(A2scaled, centers = 2, nstart = 25) 

#---------------------------PLOT THE CLUSTERS-------------------------
factoextra::fviz_cluster(km1, data = A2scaled,
             palette = c(rainbow(2)), 
             geom = "point",
             ellipse.type = "convex", 
             ggtheme = theme_bw()
)

#---------------------------CLUSTER VALIDATION------------------------
# External validation using Rand Index
# Compute the Corrected Rand Index
(cri_km1 <- external_validation(as.numeric(trainData$category), km1$cluster, 
                          method = "adjusted_rand_index", summary_stats = T))

# Compute the Variation of Information
(vi_km1 <- external_validation(as.numeric(trainData$category), km1$cluster, 
                          method = "var_info", summary_stats = F))


# CONCLUSION : the best model for k-Means algorithm is the one with unscaled data because it has higher Corrected Rand Index of 0.0001223948 and lower Variation of Information of 1.547451
```


### Clustering Algorithm 2 (Model-based Clustering)
```{r}
# set seed
set.seed(Rand)

# no scaling ---------------------------------------------------------
#-----------------------CLUSTERING USING MCLUST-----------------------
mb <- Mclust(trainData[,1:10], 2)
summary(mb)

#---------------------------PLOT THE CLUSTERS-------------------------
# Visualizing scaled 2-cluster model
fviz_mclust(mb, "classification", geom = "point",pointsize = 1.5, palette = "jama")
fviz_mclust(mb, "uncertainty", palette = "lancet")

#---------------------------CLUSTER VALIDATION------------------------
# External validation using Rand Index
# Compute the Corrected Rand Index
(cri_mb <- external_validation(as.numeric(trainData$category), mb$classification, 
                          method = "adjusted_rand_index", summary_stats = T))

# Compute the Variation of Information
(vi_mb <- external_validation(as.numeric(trainData$category), mb$classification, 
                          method = "var_info", summary_stats = F))


# with scaling -------------------------------------------------------
#-----------------------CLUSTERING USING MCLUST-----------------------
A2scaled <- scale(trainData[,1:10])

mb1 <- Mclust(A2scaled, 2)
summary(mb1)

# RESULT : the better model is the mb1 with scaled condition and two clusters because the BIC value is higher (closer to zero)

#---------------------------PLOT THE CLUSTERS-------------------------
# Visualizing scaled 2-cluster model
fviz_mclust(mb1, "classification", geom = "point",pointsize = 1.5, palette = "jama")
fviz_mclust(mb1, "uncertainty", palette = "lancet")

#---------------------------CLUSTER VALIDATION------------------------
# External validation using Rand Index
# Compute the Corrected Rand Index
(cri_mb1 <- external_validation(as.numeric(trainData$category), mb1$classification, 
                          method = "adjusted_rand_index", summary_stats = T))

# Compute the Variation of Information
(vi_mb1 <- external_validation(as.numeric(trainData$category), mb1$classification, 
                          method = "var_info", summary_stats = F))


# CONCLUSION : the best model for Model-based Clustering algorithm is the one with scaled data because it has higher BIC value (closer to zero), higher Corrected Rand Index of 0.0001717123, and lower Variation of Information of 1.547414
```


### Comparing the 2 Clustering Algorithms
```{r}
#-------------------------------k-Means-------------------------------
(cri_km <- external_validation(as.numeric(trainData$category), km$cluster, 
                          method = "adjusted_rand_index", summary_stats = T))

# The accuracy of k-means algorithm is 0.5

#------------------------Model-based Clustering-----------------------
(cri_mb1 <- external_validation(as.numeric(trainData$category), mb1$classification, 
                          method = "adjusted_rand_index", summary_stats = T))

# The accuracy of Model-based Clustering algorithm is 0.5001

#------------------------------COMPARING------------------------------
clust_algo <- c("k-Means", "Model-based Clustering")
cri <- c(cri_km, cri_mb1)
vi <- c(vi_km, vi_mb1)
(algo_table1 <- data.frame(clust_algo, cri, vi))

# CONCLUSION : the best algorithm for clustering A2 data is the Model-based Clustering algorithm with the highest accuracy of 0.5001 or 50.01%, a higher corrected rand index of 0.0001717123, and lower variation of information of 1.547414
```



## Data Classification : Using 2 Algorithms

### Classification Algorithm 1 (Naive Bayes)
```{r}
#---------------------------NBC for category--------------------------
nb_model <- NaiveBayes(category ~ ., data = trainData) #NB classifier model 
nb_pred <- predict(nb_model, testData) #test model to data testing

#--------------------Naive Bayes' Model Evaluation--------------------
# Making the Confusion Matrix
nb_table <- table(nb_pred$class, testData$category) #tabulate class
(nb_cm <- caret::confusionMatrix(nb_table)) #creating Confusion Matrix

# Plot the Confusion Matrix
testData$nb_pred <- nb_pred$class 
ggplot(testData, aes(category, nb_pred, color = category)) + 
  geom_jitter(width = 0.2, height = 0.1, size=2) + 
  labs(title = "Confusion Matrix for Naive Bayes", 
       subtitle = "Predicted vs. Observed from A2 dataset", 
       y = "Predicted", x = "Truth", caption = "by IS388A - A2 group")

# CONCLUSION : Naive Bayes classification algorithm has an accuracy of 0.8735, sensitivity of 1, and specificity of 0
```


### Algorithm 2 (Random Forest)
```{r}
# Making Random Forest model -----------------------------------------
rf_model <- randomForest(category ~ ., data = trainData, importance = TRUE)
print(rf_model)

# Variable Importance Plot for RF Model ------------------------------
varImpPlot(rf_model, main = "Variable Importance Plot")

# Making the prediction using data testing ---------------------------
rf_pred <- predict(rf_model, testData)

# Making the Confusion Matrix ----------------------------------------
(rf_cm <- caret::confusionMatrix(rf_pred, testData$category))

# Plot the Confusion Matrix ------------------------------------------
testData$rf_pred <- rf_pred
ggplot(testData, aes(category, rf_pred, color = category)) + 
  geom_jitter(width = 0.2, height = 0.1, size=2) + 
  labs(title = "Confusion Matrix for Random Forest", 
       subtitle = "Predicted vs. Observed from A2 dataset", 
       y = "Predicted", x = "Truth", caption = "by IS388A - A2 group")

# CONCLUSION : Random Forest classification algorithm has an accuracy of 0.9805, sensitivity of 1, and specificity of 0.8458

```


### Comparing the 2 Classification Algorithms
```{r}
#-----------------------------Naive Bayes-----------------------------
nb_cm

#----------------------------Random Forest----------------------------
rf_cm

#------------------------------COMPARING------------------------------
classi_algo <- c("Naive Bayes", "Random Forest")
accuracy <- c(nb_cm$overall[1], rf_cm$overall[1])
kappa <- c(nb_cm$overall[2], rf_cm$overall[2])
sensitivity <- c(nb_cm$byClass[1], rf_cm$byClass[1])
specificity <- c(nb_cm$byClass[2], rf_cm$byClass[2])
precision <- c(nb_cm$byClass[5], rf_cm$byClass[5])

(algo_table2 <- data.frame(classi_algo, accuracy, kappa, sensitivity, specificity, precision))

# CONCLUSION : the best algorithm for classifying A2 dataset is Random Forest algorithm with the highest accuracy of 0.9805, kappa value of 0.9055, sensitivity of 1, specificity of 0.8458, and precision of 0.9781

```



## Data Prediction

### Prediction Algorithm (Linear and Multiple Linear Regression)
#### Linear Regression
```{r}
train <- trainData[, -c(11)]
test <- testData[, -c(11)]

#----------------------CORRELATION CALCULATION------------------------
# Checking the correlation between numerical variables
cor(train, method = "pearson")

# Variable with the strongest correlation with Price variable is squareMeters with a correlation value of 0.999999351 

#-----------------------LINEAR REGRESSION MODEL-----------------------
# Price variable as dependent variable and squareMeters as predictor/independent variable
(linreg_model <- lm(price ~ squareMeters, data = train))
summary(linreg_model)

#i. Model signifikan karena p-value 2.2e-16 < alpha 5% atau 0.05

#ii. variabilitas price yang dapat dijelaskan oleh variabilitas squareMeters pada model adalah sebesar 100% (r kuadrat = 1)

#iii. penambahan price setiap penambahan 1 unit squareMeters adalah 1.000e+02

#----------------CHECKING MODEL DIAGNOSIS AND ASSUMPTION--------------
# Regression diagnosis with plot
par(mfrow = c(2,3))
plot(linreg_model, which = 1:6) 

# Checking assumptions
par(mfrow = c(1,1))

# Autocorrelation test with Durbin-Watson
car::durbinWatsonTest(linreg_model)

# Multicollinearity test with VIF
# car::vif(linreg_model)

# Homoscedasticity test with Breusch-Pagan 
lmtest::bptest(linreg_model)

# Residual normality test with Anderson-Darling test
nortest::ad.test(linreg_model$residuals) 

# RESULT :
# uji otokorelasi 
# Asumsi terpenuhi : nilai p-value 0 > alpha 0.05
# uji multikolinearitas 
# Uji multikolinearitas tidak dapat dilakukan karena linreg_model hanya terdiri dari 2 variabel saja
# uji homoskedastisitas 
# Asumsi terpenuhi : nilai p-value 0.8509 > alpha 0.05 -> varians residual homoskedastis
# uji normalitas 
# Asumsi tidak terpenuhi : nilai p-value 2.2e-16 < alpha 0.05 -> data tidak mengikuti distribusi gaussian

#-------------------------MODEL PREDICTION----------------------------
linreg_pred <- predict(linreg_model, data.frame(squareMeters = test$squareMeters))

linreg_pred

# Calculating RMSE value
(linreg_rmse <- Metrics::rmse(test$price, linreg_pred))

# RESULT : The linear regression model has a RMSE value of 3239.512
```


#### Multiple Linear Regression
```{r}
#---------------------MULTILINEAR REGRESSION MODEL--------------------
# Price variable as dependent variable and the rest as predictor/independent variables
(mulinreg_model <- lm(price ~ ., data = train))
summary(mulinreg_model)

# The only significant variables are squareMeters and floors
# So, we make new model using price variable as dependent variable and squareMeters + floors as predictor/independent variables
(mulinreg_model1 <- lm(price ~ squareMeters + floors, data = train))
summary(mulinreg_model1)

#i. Model signifikan karena p-value 2.2e-16 < alpha 5% atau 0.05

#ii. variabilitas price yang dapat dijelaskan oleh variabilitas variabel independen pada model adalah sebesar 100% (r kuadrat = 1)

#iii. penambahan price setiap penambahan 1 unit squareMeters adalah 1.000e+02
#     penambahan price setiap penambahan 1 unit floors adalah 5.512e+01

#----------------CHECKING MODEL DIAGNOSIS AND ASSUMPTION--------------
# Regression diagnosis with plot
par(mfrow = c(2,3))
plot(mulinreg_model1, which = 1:6) 

# Checking assumptions
par(mfrow = c(1,1))

# Autocorrelation test with Durbin-Watson
car::durbinWatsonTest(mulinreg_model1)

# Multicollinearity test with VIF
car::vif(mulinreg_model1)

# Homoscedasticity test with Breusch-Pagan 
lmtest::bptest(mulinreg_model1)

# Residual normality test with Anderson-Darling test
nortest::ad.test(mulinreg_model1$residuals) 

# RESULT :
# uji otokorelasi 
# Asumsi terpenuhi : nilai p-value 0 > alpha 0.05
# uji multikolinearitas 
# Asumsi terpenuhi : nilai VIF berada di rentang 1-5
# uji homoskedastisitas 
# Asumsi tidak terpenuhi : nilai p-value 2.2e-16 < alpha 0.05 -> varians residual tidak homoskedastis
# uji normalitas 
# Asumsi tidak terpenuhi : nilai p-value 2.2e-16 < alpha 0.05 -> data tidak mengikuti distribusi gaussian

#-------------------------MODEL PREDICTION----------------------------
mulinreg_pred <- predict(mulinreg_model1, newdata = test)

mulinreg_pred

# Calculating RMSE value
(mulinreg_rmse <- Metrics::rmse(test$price, mulinreg_pred))

# RESULT : The multiple linear regression model has a RMSE value of 2835.999
```


#### Choosing the best model from Prediction Algorithm 1
```{r}
#----------------------Linear Regression Model------------------------
linreg_rmse

#------------------Multiple Linear Regression Model-------------------
mulinreg_rmse

#----------------------COMPARING THE RMSE VALUES----------------------
pred_algo <- c("Linear Regression", "Multiple Linear Regression")
RMSE <- c(linreg_rmse, mulinreg_rmse)

(algo_table3 <- data.frame(pred_algo, RMSE))

# CONCLUSION : The most suitable prediction model for this A2 dataset is mulinreg_model or multiple linear regression model because it has the smallest value of RMSE (the difference between the actual value and the predicted value) between the two models of 2835.999
```


#### Making Prediction Model
```{r}
# Predict the house price if the number of floors is 100 and the area is 5.000 m2
(p1 <- predict(mulinreg_model1, data.frame(floors = 100, squareMeters = 5000)))

(p2 <- predict(mulinreg_model1, data.frame(floors = 50, squareMeters = 1000)))

# So, the house price will be 509175.5 
```